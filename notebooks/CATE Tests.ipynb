{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d447a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, joblib, wandb, json\n",
    "from collections import defaultdict\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import torch, wandb\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from econml import dr, dml, metalearners\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVC, SVR, LinearSVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from src.data.data_module import LBIDD, IHDP, Twins, Synth\n",
    "from src.models.nce_ite import NCE\n",
    "from src.models.cate_model import CATEModel\n",
    "from src.models.benchmarks import AE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "## for Palatino and other serif fonts use:\n",
    "#rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896df7c",
   "metadata": {},
   "source": [
    "# Exp. prep.\n",
    "\n",
    "You need to run the below 2 cells regardless of what experiment. These cells load the learnt representations (with parameters at top of first cell), and prep the CATE learners. \n",
    "\n",
    "Of course, before loading representations, you first have to learn them + upload to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181771f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to unpickle estimator SVC from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator KernelRidge from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator KMeans from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "data = 'ihdp'                         # or 'twins' or 'synth'\n",
    "project = f'nce-ite-{data}'           # make sure this matches your W&B project\n",
    "\n",
    "train_size=500                        # set your training set size\n",
    "fix=True                              # fix your dimensions regardless of training (note this is only possible for synth. data)\n",
    "n_models=10                           # amount of different trained models\n",
    "fix_K = False                         # keep your K fixed (use for covariance checks)\n",
    "\n",
    "varying_sweeps = {                    # select your sweep number for correct training size:\n",
    "    100: 'e4cmxk15',                  #    {dim: sweep_id, ...}\n",
    "    250: 't4bw8nvi',\n",
    "    500: '3ii8nsyi',\n",
    "    1000: 'wz29ayzz',\n",
    "    1500: 'hwu8y0u6',\n",
    "}\n",
    "\n",
    "fixed_sweeps = {\n",
    "    100: 'ajt2xpzj', \n",
    "    250: 'ncxqa6kx', \n",
    "    500: 'epv99a73', \n",
    "    1000:'t3z4442e', \n",
    "    1500:'ulralaif', \n",
    "}\n",
    "\n",
    "twin_sweeps = {\n",
    "    500: 'rgrvgmmm',\n",
    "    1000:'j8fv1g7f',\n",
    "    1500:'zq5io55q',\n",
    "    2000: 'fkch1ixg',\n",
    "    2500: 'l66oyh02',\n",
    "    5000: 'xv3krdpx',\n",
    "    10000:'5h9so1uc',\n",
    "}\n",
    "\n",
    "twins_sweeps_fixed_K = {\n",
    "    500: '1e30plop',\n",
    "    1000: '03ixmhhc',\n",
    "    1500: 'y0diivjg',\n",
    "    2000: 'ehu52p5o',\n",
    "    2500: 'xwamjm77',\n",
    "    5000: 'xv3krdpx',\n",
    "    10000:'5h9so1uc',\n",
    "}\n",
    "\n",
    "ihdp_sweeps = {\n",
    "    100: 'ozt3qms4',\n",
    "    250: 'g5qa43de',\n",
    "    500: 'hz03jfu7',\n",
    "}\n",
    "\n",
    "\n",
    "if data == 'synth':\n",
    "    sweep_name = fixed_sweeps[train_size] if fix else varying_sweeps[train_size]\n",
    "elif data == 'twins':\n",
    "    sweep_name = twin_sweeps[train_size] if not fix_K else twins_sweeps_fixed_K[train_size]\n",
    "elif data == 'ihdp':\n",
    "    sweep_name = ihdp_sweeps[train_size]\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "data_seed_standard = 524\n",
    "data_seeds = defaultdict(lambda: data_seed_standard)\n",
    "data_seeds[100] = 979\n",
    "data_seeds[500] = 255\n",
    "\n",
    "if data == 'ihdp':\n",
    "    dm = IHDP(batch_size=256, limit_train_size=train_size)\n",
    "elif data == 'lbidd':\n",
    "    dm =  LBIDD(\n",
    "        batch_size=256, id='93aab00aeb234a3b985eeb32e04a353d',\n",
    "        location='../data/LBIDD', limit_train_size=train_size)\n",
    "elif data == 'twins':\n",
    "    dm = Twins(batch_size=256, limit_train_size=train_size)\n",
    "elif data == 'synth':\n",
    "    dims = {\n",
    "        100: 50,\n",
    "        250: 100,\n",
    "        500: 150,\n",
    "        1000: 200,\n",
    "        1500: 250,\n",
    "    }\n",
    "    \n",
    "    if fix:\n",
    "        dim = 100\n",
    "    else:\n",
    "        dim = dims[train_size]\n",
    "\n",
    "    dm = Synth(\n",
    "        batch_size=64,\n",
    "        use_existing_data=False,\n",
    "        n=train_size,\n",
    "        seed=data_seeds[train_size],\n",
    "        dim=dim,\n",
    "        scale_treatment_balance=1)\n",
    "else:\n",
    "    raise ValueError('Error value on --data. Please give one of \"ihdp\", \"lbidd\", or \"twins\".')\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup(stage='test')\n",
    "dm.setup(stage='fit')\n",
    "\n",
    "X_train = dm.train[dm.x_cols].copy(deep=True)\n",
    "Z_train = dm.train.z.copy(deep=True)\n",
    "Y_train = dm.train.y.copy(deep=True)\n",
    "\n",
    "X_test = dm.test[dm.x_cols].copy(deep=True)\n",
    "Y0_test= dm.test.y0.copy(deep=True)\n",
    "Y1_test= dm.test.y1.copy(deep=True)\n",
    "\n",
    "Y_train[Z_train.to_numpy() == 1] += 1.\n",
    "Y1_test += 1.\n",
    "\n",
    "    \n",
    "# LOAD MODELS\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(path=f'jeroenbe/{project}/{sweep_name}') #nce-ite-lbidd\n",
    "                \n",
    "summary_list = []\n",
    "for run in sweep.runs:\n",
    "    if run.state == \"finished\":\n",
    "        d = run.summary._json_dict\n",
    "        d['id'] = run.id\n",
    "        summary_list.append(d)\n",
    "                        \n",
    "\n",
    "summary_df = pd.DataFrame.from_records(summary_list)\n",
    "model_ids = summary_df.nsmallest(n=n_models, columns=['PEHE with representation']).id.to_numpy().astype(str)\n",
    "\n",
    "models = dict()\n",
    "for model_id in model_ids:\n",
    "    try:\n",
    "        params = wandb.restore(f'nce.ckpt.ckpt', run_path=f'jeroenbe/{project}/{model_id}', replace=True)\n",
    "    except:\n",
    "        params = wandb.restore(f'nce.ckpt-v0.ckpt', run_path=f'jeroenbe/{project}/{model_id}', replace=True)\n",
    "    \n",
    "    models[model_id] = NCE.load_from_checkpoint(params.name).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43090474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build several combinations for CATE learners, \n",
    "# please make sure all combinations match to those reported\n",
    "# in our paper, before reproducing our results.\n",
    "# For example, the regressor() function should return the\n",
    "# regressor that is reported in our paper. Some examples\n",
    "# are provided below.\n",
    "\n",
    "\n",
    "\n",
    "cate_amount = n_models\n",
    "\n",
    "kwargs = {\n",
    "            #'max_depth': 40,\n",
    "            #'n_estimators': 100,\n",
    "            'max_iter': 50000,\n",
    "            'tol': .01\n",
    "        }\n",
    "\n",
    "\n",
    "def regressor():\n",
    "    return Ridge()\n",
    "    return KernelRidge()\n",
    "    return Pipeline([\n",
    "        #('poly', PolynomialFeatures(degree=2)),\n",
    "        ('power', PowerTransformer()),\n",
    "        ('regr', Ridge(alpha=.001))\n",
    "    ])\n",
    "\n",
    "def classifier():\n",
    "    return SVC(probability=True)\n",
    "\n",
    "def get_cate_learners(amount):\n",
    "    \n",
    "    return {\n",
    "        'X': lambda: [\n",
    "            metalearners.XLearner(models= regressor(),\n",
    "                propensity_model=classifier(),\n",
    "                cate_models= regressor()) for _ in range(amount)],\n",
    "        'DR': lambda: [\n",
    "            dr.LinearDRLearner(\n",
    "                model_propensity=classifier(),\n",
    "                model_regression= regressor()) for _ in range(amount)],\n",
    "        'S': lambda: [\n",
    "            metalearners.SLearner(overall_model= regressor()) for _ in range(amount)],\n",
    "        'T': lambda: [\n",
    "            metalearners.TLearner(models= regressor()) for _ in range(amount)],\n",
    "        'R': lambda: [dml.NonParamDML(\n",
    "                model_y=regressor(),\n",
    "                model_t=classifier(),\n",
    "                model_final= KernelRidge(),\n",
    "                discrete_treatment=True) for _ in range(amount)],    \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d1b5c",
   "metadata": {},
   "source": [
    "# Test CATE learners with and without EBM\n",
    "\n",
    "These cells need only executing when interested in CATE performance with and without EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f418a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dict()\n",
    "cate_learners = get_cate_learners(4)\n",
    "for k, v in cate_learners.items():\n",
    "    out[k] = {\n",
    "        'with representation': [],\n",
    "        'without representation': [],\n",
    "        'with latent': [],\n",
    "    }\n",
    "    for _, nce in models.items():\n",
    "        cate_learner = v()\n",
    "        cate_nr = CATEModel(model=cate_learner[0], representation=None, standardize=False)\n",
    "        cate_r = CATEModel(model=cate_learner[1], representation=nce, standardize=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        cate_nr.fit(X_train, Z_train, Y_train)\n",
    "        cate_r.fit(X_train, Z_train, Y_train)\n",
    "        \n",
    "        \n",
    "        pehe_no_repr = cate_nr.eval(X_test, Y0_test, Y1_test)\n",
    "        pehe_repr = cate_r.eval(X_test, Y0_test, Y1_test)\n",
    "\n",
    "        \n",
    "        out[k]['with representation'].append(pehe_repr)\n",
    "        out[k]['without representation'].append(pehe_no_repr)\n",
    "        \n",
    "        \n",
    "        if data == 'synth':\n",
    "            cate_u = CATEModel(model=cate_learner[2], representation=None, standardize=False)\n",
    "            \n",
    "            U_train = dm.train_u\n",
    "            mu_u = U_train.mean(axis=0)\n",
    "            std_u = U_train.std(axis=0)\n",
    "            U_train -= mu_u\n",
    "            U_train /= std_u\n",
    "        \n",
    "            cate_u.fit(pd.DataFrame(dm.train_u), Z_train, Y_train)\n",
    "        \n",
    "            U_test = dm.test_u\n",
    "            U_test -= mu_u\n",
    "            U_test /= std_u\n",
    "            \n",
    "            pehe_u = cate_u.eval(pd.DataFrame(dm.test_u), Y0_test, Y1_test)\n",
    "            out[k]['with latent'].append(pehe_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93cf8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"X\": {\n",
      "        \"with representation\": [\n",
      "            1.619916425156085,\n",
      "            0.0052781109929950915\n",
      "        ],\n",
      "        \"without representation\": [\n",
      "            1.5644931207443868,\n",
      "            1.8417375312065561e-06\n",
      "        ],\n",
      "        \"latent\": [\n",
      "            NaN,\n",
      "            NaN\n",
      "        ]\n",
      "    },\n",
      "    \"DR\": {\n",
      "        \"with representation\": [\n",
      "            1.6349975347585122,\n",
      "            0.05064465206539426\n",
      "        ],\n",
      "        \"without representation\": [\n",
      "            1.690497780248042,\n",
      "            0.04683625822346018\n",
      "        ],\n",
      "        \"latent\": [\n",
      "            NaN,\n",
      "            NaN\n",
      "        ]\n",
      "    },\n",
      "    \"S\": {\n",
      "        \"with representation\": [\n",
      "            1.6232791986577861,\n",
      "            0.0015007790764973424\n",
      "        ],\n",
      "        \"without representation\": [\n",
      "            1.6370527343705024,\n",
      "            2.220446049250313e-16\n",
      "        ],\n",
      "        \"latent\": [\n",
      "            NaN,\n",
      "            NaN\n",
      "        ]\n",
      "    },\n",
      "    \"T\": {\n",
      "        \"with representation\": [\n",
      "            1.6174386049494625,\n",
      "            0.017179055473209755\n",
      "        ],\n",
      "        \"without representation\": [\n",
      "            1.5673694600920594,\n",
      "            2.220446049250313e-16\n",
      "        ],\n",
      "        \"latent\": [\n",
      "            NaN,\n",
      "            NaN\n",
      "        ]\n",
      "    },\n",
      "    \"R\": {\n",
      "        \"with representation\": [\n",
      "            1.7626226599329466,\n",
      "            0.14458285205067747\n",
      "        ],\n",
      "        \"without representation\": [\n",
      "            1.932839882776038,\n",
      "            0.0645293250783949\n",
      "        ],\n",
      "        \"latent\": [\n",
      "            NaN,\n",
      "            NaN\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean of empty slice.\n",
      "invalid value encountered in double_scalars\n",
      "Degrees of freedom <= 0 for slice\n",
      "invalid value encountered in true_divide\n",
      "invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "for k, v in out.items():\n",
    "    wr = 'with representation'\n",
    "    wor = 'without representation'\n",
    "    results[k] = {\n",
    "        wr: (np.mean(out[k][wr]), np.std(out[k][wr])),\n",
    "        wor: (np.mean(out[k][wor]), np.std(out[k][wor])),\n",
    "        'latent': (np.mean(out[k]['with latent']), np.std(out[k]['with latent'])),\n",
    "    }\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5b4de",
   "metadata": {},
   "source": [
    "# Different Dim-red.\n",
    "Here we train and compare with various dimensionality reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db69e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIM RED.s:\n",
    "# -> AE\n",
    "# -> PCA\n",
    "# -> FeatureAgglomeration\n",
    "# -> TruncatedSVD\n",
    "\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.decomposition import TruncatedSVD, KernelPCA\n",
    "from sklearn.manifold import TSNE, SpectralEmbedding, Isomap, LocallyLinearEmbedding, MDS\n",
    "\n",
    "\n",
    "class DIM_WRAPPER:\n",
    "    def __init__(self, dm_red):\n",
    "        self.dm_red = dm_red\n",
    "        self.device='cpu'\n",
    "    def __call__(self, x):\n",
    "        if getattr(self.dm_red, \"transform\", None) is not None:\n",
    "            return self.dm_red.transform(x)\n",
    "        else:\n",
    "            return self.dm_red.fit_transform(x)\n",
    "\n",
    "\n",
    "dimred_out = dict()\n",
    "cate_learners = get_cate_learners(8)\n",
    "i = 0\n",
    "for k, v in cate_learners.items():\n",
    "    dimred_out[k] = {\n",
    "        'with EBM': [],\n",
    "        'with PCA': [],\n",
    "        'with FA': [],\n",
    "        'with SE': [],\n",
    "        'with IM': [],\n",
    "        'with KernelPCA': [],\n",
    "        'with AE': [],\n",
    "        'with latent': [],\n",
    "    }\n",
    "    for _, nce in models.items():\n",
    "        cate_learner = v()\n",
    "        \n",
    "        print('now on PCA')\n",
    "        pca_ = PCA(n_components=nce.K)\n",
    "        pca_.fit(X_train)\n",
    "        pca = DIM_WRAPPER(pca_)\n",
    "        \n",
    "        print('now on FA')\n",
    "        fa_ = FeatureAgglomeration(n_clusters=nce.K)\n",
    "        fa_.fit(X_train)\n",
    "        fa = DIM_WRAPPER(fa_)\n",
    "        \n",
    "        print('now on SE')\n",
    "        se_ = SpectralEmbedding(n_components=nce.K)\n",
    "        se_.fit(X_train)\n",
    "        se = DIM_WRAPPER(se_)\n",
    "        \n",
    "        print('now on IM')\n",
    "        im_ = Isomap(n_neighbors=nce.K)\n",
    "        im_.fit(X_train)\n",
    "        im = DIM_WRAPPER(im_)\n",
    "        \n",
    "        print('now on LLE')\n",
    "        lle_ = KernelPCA(n_components=nce.K, kernel='rbf')\n",
    "        lle_ = lle_.fit(X_train)\n",
    "        lle = DIM_WRAPPER(lle_)\n",
    "        \n",
    "        print('now on AE')\n",
    "        if data == 'synth':\n",
    "            pl.utilities.seed.seed_everything(i * dm.size(1))\n",
    "            i+=1\n",
    "        ae = AE(\n",
    "            input_dim = dm.size(1), \n",
    "            K=nce.K, \n",
    "            lr=nce.lr, architecture=[(40,40) for _ in range(6)])\n",
    "        trainer = Trainer(callbacks=[EarlyStopping(monitor='val_loss')], max_epochs=50)\n",
    "        trainer.fit(ae, dm)\n",
    "        \n",
    "        print('Dim red. Done')\n",
    "        \n",
    "        cate_r = CATEModel(model=cate_learner[0], representation=nce, standardize=False)\n",
    "        cate_pca = CATEModel(model=cate_learner[1], representation=pca, standardize=False)\n",
    "        cate_fa = CATEModel(model=cate_learner[2], representation=fa, standardize=False)\n",
    "        cate_se = CATEModel(model=cate_learner[3], representation=se, standardize=False)\n",
    "        cate_im = CATEModel(model=cate_learner[5], representation=im, standardize=False)\n",
    "        cate_lle = CATEModel(model=cate_learner[4], representation=lle, standardize=False)\n",
    "        cate_ae = CATEModel(model=cate_learner[6], representation=ae, standardize=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('learn EBM')\n",
    "        cate_r.fit(X_train, Z_train, Y_train)\n",
    "        print('learn PCA')\n",
    "        cate_pca.fit(X_train, Z_train, Y_train)\n",
    "        print('learn FA')\n",
    "        cate_fa.fit(X_train, Z_train, Y_train)\n",
    "        print('learn SE')\n",
    "        cate_se.fit(X_train, Z_train, Y_train)\n",
    "        print('learn IM')\n",
    "        cate_im.fit(X_train, Z_train, Y_train)\n",
    "        print('learn MDS')\n",
    "        cate_lle.fit(X_train, Z_train, Y_train)\n",
    "        print('learn AE')\n",
    "        cate_ae.fit(X_train, Z_train, Y_train)\n",
    "        \n",
    "        print('eval EBM')\n",
    "        pehe_repr = cate_r.eval(X_test, Y0_test, Y1_test)\n",
    "        print('eval PCA')\n",
    "        pehe_pca = cate_pca.eval(X_test, Y0_test, Y1_test)\n",
    "        print('eval FA')\n",
    "        pehe_fa = cate_fa.eval(X_test, Y0_test, Y1_test)\n",
    "        print('eval SE')\n",
    "        pehe_se = cate_se.eval(X_test.iloc[:2000,:], Y0_test.iloc[:2000], Y1_test.iloc[:2000])\n",
    "        print('eval IM')\n",
    "        pehe_im = cate_im.eval(X_test, Y0_test, Y1_test)\n",
    "        print('eval MDS')\n",
    "        pehe_lle = cate_lle.eval(X_test, Y0_test, Y1_test)\n",
    "        print('eval AE')\n",
    "        pehe_ae = cate_ae.eval(X_test, Y0_test, Y1_test)\n",
    "\n",
    "        \n",
    "        dimred_out[k]['with EBM'].append(pehe_repr)\n",
    "        dimred_out[k]['with PCA'].append(pehe_pca)\n",
    "        dimred_out[k]['with FA'].append(pehe_fa)\n",
    "        dimred_out[k]['with SE'].append(pehe_se)\n",
    "        dimred_out[k]['with IM'].append(pehe_im)\n",
    "        dimred_out[k]['with KernelPCA'].append(pehe_lle)\n",
    "        dimred_out[k]['with AE'].append(pehe_ae)\n",
    "        \n",
    "        \n",
    "        if data == 'synth':\n",
    "            cate_u = CATEModel(model=cate_learner[-1], representation=None, standardize=False)\n",
    "            \n",
    "            U_train = dm.train_u\n",
    "            mu_u = U_train.mean(axis=0)\n",
    "            std_u = U_train.std(axis=0)\n",
    "            U_train -= mu_u\n",
    "            U_train /= std_u\n",
    "        \n",
    "            cate_u.fit(pd.DataFrame(dm.train_u), Z_train, Y_train)\n",
    "        \n",
    "            U_test = dm.test_u\n",
    "            U_test -= mu_u\n",
    "            U_test /= std_u\n",
    "            \n",
    "            pehe_u = cate_u.eval(pd.DataFrame(dm.test_u), Y0_test, Y1_test)\n",
    "            dimred_out[k]['with latent'].append(pehe_u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04569959",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for k, v in dimred_out.items():\n",
    "    results[k] = {\n",
    "        \n",
    "        'with PCA': (np.mean(dimred_out[k]['with PCA']), np.std(dimred_out[k]['with PCA'])),\n",
    "        'with FA': (np.mean(dimred_out[k]['with FA']), np.std(dimred_out[k]['with FA'])),\n",
    "        'with SE': (np.mean(dimred_out[k]['with SE']), np.std(dimred_out[k]['with SE'])),\n",
    "        'with IM': (np.mean(dimred_out[k]['with IM']), np.std(dimred_out[k]['with IM'])),\n",
    "        'with KernelPCA': (np.mean(dimred_out[k]['with KernelPCA']), np.std(dimred_out[k]['with KernelPCA'])),\n",
    "        'with AE': (np.mean(dimred_out[k]['with AE']), np.std(dimred_out[k]['with AE'])),\n",
    "        'with EBM': (np.mean(dimred_out[k]['with EBM']), np.std(dimred_out[k]['with EBM'])),\n",
    "        'latent': (np.mean(dimred_out[k]['with latent']), np.std(dimred_out[k]['with latent'])),\n",
    "    }\n",
    "print(json.dumps(results, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nce-ite)",
   "language": "python",
   "name": "nce-ite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
